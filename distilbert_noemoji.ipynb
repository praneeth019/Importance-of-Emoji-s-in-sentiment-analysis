{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import TrainerCallback, PrinterCallback\n",
    "\n",
    "#\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "#\n",
    "import torch\n",
    "\n",
    "def read_data(data_path):\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    sampled_data = pd.read_csv(data_path, encoding='latin1')\n",
    "\n",
    "    # Replace all occurrences of \"4\" with \"1\" in the labels\n",
    "    sampled_data.iloc[:, 0].replace(4, 1, inplace=True)\n",
    "\n",
    "\n",
    "    # Extract the first column into a separate array\n",
    "    labels = sampled_data.iloc[:, 0].values\n",
    "\n",
    "    # Print all unique values in the labels\n",
    "    unique_labels = sampled_data.iloc[:, 0].unique()\n",
    "    print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "    # Extract the last column into a separate array\n",
    "    tweets = sampled_data.iloc[:, -1].values\n",
    "\n",
    "    return labels, tweets\n",
    "\n",
    "def split_into_train_dev_test(labels, tweets, test_size=0.1, dev_size=0.1, random_state=42):\n",
    "    # Split the data into training and testing sets\n",
    "    train_tweets, test_tweets, train_labels, test_labels = train_test_split(\n",
    "        tweets, labels, test_size=test_size, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Further split the training data into training and development sets\n",
    "    train_tweets, dev_tweets, train_labels, dev_labels = train_test_split(\n",
    "        train_tweets, train_labels, test_size=dev_size, shuffle=True, random_state=random_state)\n",
    "\n",
    "    return train_tweets, dev_tweets, test_tweets, train_labels, dev_labels, test_labels\n",
    "\n",
    "\n",
    "def encode_using_bert_tokenizer(train_tweets, test_tweets, dev_tweets):\n",
    "    train_tweets = [str(tweet) for tweet in train_tweets]\n",
    "    test_tweets = [str(tweet) for tweet in test_tweets]\n",
    "    dev_tweets = [str(tweet) for tweet in dev_tweets]\n",
    "\n",
    "    print(\"Attempting to tokenize...\")\n",
    "    train_encodings = tokenizer(train_tweets, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_tweets, truncation=True, padding=True)\n",
    "    dev_encodings = tokenizer(dev_tweets, truncation=True, padding=True)\n",
    "\n",
    "    train_encodings = dict(train_encodings)\n",
    "    test_encodings = dict(test_encodings)\n",
    "    dev_encodings = dict(dev_encodings)\n",
    "    print(\"Successfully tokenized.\")\n",
    "\n",
    "    return train_encodings, test_encodings, dev_encodings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolarityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         # Creating the item dictionary\n",
    "\n",
    "        item = {}\n",
    "        item['input_ids'] = torch.tensor(self.encodings['input_ids'][idx])\n",
    "        item['attention_mask'] = torch.tensor(self.encodings['attention_mask'][idx])\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class CustomPrinterCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"Training begins!\")\n",
    "\n",
    "    def on_train_batch_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            print(f\"Step {state.global_step}:\")\n",
    "            print(f\"  Train loss: {state.log_history[-1]['loss']}\")\n",
    "            print(f\"  Train accuracy: {state.log_history[-1]['accuracy'] * 100:.2f}%\")\n",
    "\n",
    "def train_and_evaluate(train_dataset, val_dataset, test_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=3,              # total number of training epochs\n",
    "        per_device_train_batch_size=6,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset,            # evaluation dataset\n",
    "        callbacks=[CustomPrinterCallback()]  # custom printer callback\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predicted_labels = predictions.predictions.argmax(axis=1)\n",
    "    true_labels = test_dataset['labels']  # Assuming 'label' is the column name for labels in the test dataset\n",
    "\n",
    "    # Calculate metrics\n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "#this is the data set sampled from the bigger data size\n",
    "#it has a size of 10K\n",
    "#This file path should be the file that path\n",
    "#that is modified for the other trainings\n",
    "path = 'path to data'\n",
    "labels, tweets = read_data(path)\n",
    "#split into train, test, and dev set\n",
    "train_tweets, dev_tweets, test_tweets, train_labels, dev_labels, test_labels = split_into_train_dev_test(labels, tweets)\n",
    "\n",
    "#encode using the bert tokenizer\n",
    "train_encodings, test_encodings, dev_encodings = encode_using_bert_tokenizer(train_tweets, test_tweets, dev_tweets)\n",
    "\n",
    "#create dataset\n",
    "train_dataset = PolarityDataset(train_encodings, train_labels)\n",
    "dev_dataset = PolarityDataset(dev_encodings, dev_labels)\n",
    "test_dataset = PolarityDataset(test_encodings, test_labels)\n",
    "\n",
    "\n",
    "#access the report and display it for visualization\n",
    "report = train_and_evaluate(train_dataset, dev_dataset, test_dataset)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model fine-tuned for sentiment analysis on Twitter data\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Example tweet\n",
    "tweet = \"susan boyle come second place! she deserve to win!<3 ðŸ˜‰\"\n",
    "\n",
    "# Make a prediction\n",
    "result = sentiment_analysis(tweet)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
